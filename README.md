Real-time Sign Language Recognition (ASL to Text/Speech)



ğŸ“Œ Overview

This project implements a real-time sign language recognition system that converts American Sign Language (ASL) gestures into text and speech. The system is designed for low-latency inference (â‰¤ 100ms/frame) and can run on both cloud and edge devices.

ğŸš€ Features

Real-time Gesture Recognition: Detects and classifies ASL signs from live webcam input.

Fast & Optimized Inference: Uses TensorRT and ONNX for acceleration.

Multi-Device Support: Runs on CPU, GPU, and edge devices (Jetson Nano, Raspberry Pi, etc.).

Extensible Model Architecture: Supports MediaPipe + Transformer, 3D CNN, and ST-GCN.

REST API & Web Interface: Provides a FastAPI backend and a React-based web client.

Comprehensive MLOps Pipeline: Includes data versioning (DVC), experiment tracking (MLflow), and CI/CD (GitHub Actions).

ğŸ“‚ Project Structure

ğŸ“¦ sign-language-recognition
â”œâ”€â”€ ğŸ“ data                # Datasets and annotations
â”œâ”€â”€ ğŸ“ models              # Pretrained models & training scripts
â”œâ”€â”€ ğŸ“ src                 # Core source code (detection, tracking, inference)
â”‚   â”œâ”€â”€ inference.py       # Real-time inference pipeline
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â”œâ”€â”€ preprocess.py      # Data preprocessing
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”œâ”€â”€ ğŸ“ deployment          # Deployment scripts (Docker, Kubernetes, Jetson Nano)
â”œâ”€â”€ ğŸ“ webapp              # Web-based frontend (React.js)
â”œâ”€â”€ ğŸ“„ requirements.txt    # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md           # Project documentation
â”œâ”€â”€ ğŸ“„ LICENSE             # License file
â””â”€â”€ ğŸ“„ .gitignore          # Git ignore file

ğŸ› ï¸ Installation

1ï¸âƒ£ Clone the Repository

git clone https://github.com/yourusername/sign-language-recognition.git
cd sign-language-recognition

2ï¸âƒ£ Set Up the Environment

pip install -r requirements.txt

For GPU acceleration, install TensorRT:

pip install nvidia-tensorrt

3ï¸âƒ£ Run the Application

python src/inference.py --source webcam

ğŸ§  Model Architecture

The system employs multiple architectures for gesture recognition:

MediaPipe Holistic + Transformer (Fast, real-time, lower accuracy)

ST-GCN (Spatial-Temporal Graph CNN) (Best for hand-body relations)

3D CNN (I3D, SlowFast) (End-to-end video-based recognition)

ğŸš€ Deployment

ğŸ–¥ï¸ Local Deployment

uvicorn src.api:app --host 0.0.0.0 --port 8000

Access API at: http://localhost:8000/docs

ğŸ³ Docker Deployment

docker build -t asl-recognition .
docker run -p 8000:8000 asl-recognition

â˜ï¸ Cloud Deployment (AWS Lambda)

cd deployment/aws
sh deploy.sh

ğŸ“Š Experiment Tracking & Monitoring

MLflow: Tracks model training metrics.

Prometheus + Grafana: Monitors real-time inference latency.

Evidently AI: Detects data drift.

ğŸ“š References

MediaPipe Holistic

ST-GCN Paper

TensorRT Optimization

ğŸ”— Contributing

We welcome contributions! Please follow our CONTRIBUTING.md guide.

ğŸ“œ License

This project is licensed under the MIT License. See LICENSE for details.

